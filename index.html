<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link href='https://fonts.googleapis.com/css?family=Chivo:900' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-dark.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <title>SimpleRock by CyberAnalyticDevTeam</title>
  </head>

  <body>
    <div id="container">
      <div class="inner">

        <header>
          <h1>SimpleRock</h1>
          <h2>A Chef cookbook that builds an open source version of MOCYBER&#39;s ROCK (Response Operation Collection Kit) platform. </h2>
        </header>

        <section id="downloads" class="clearfix">
          <a href="https://github.com/CyberAnalyticDevTeam/SimpleRock/zipball/master" id="download-zip" class="button"><span>Download .zip</span></a>
          <a href="https://github.com/CyberAnalyticDevTeam/SimpleRock/tarball/master" id="download-tar-gz" class="button"><span>Download .tar.gz</span></a>
          <a href="https://github.com/CyberAnalyticDevTeam/SimpleRock" id="view-on-github" class="button"><span>View on GitHub</span></a>
        </section>

        <hr>

        <section id="main_content">
          <h2>
<a id="response-operation-collections-kit-reference-build" class="anchor" href="#response-operation-collections-kit-reference-build" aria-hidden="true"><span class="octicon octicon-link"></span></a>Response Operation Collections Kit Reference Build</h2>

<p>This build was created and tested using CentOS 7.2. I pretty much guarantee that it won't work with anything else other than RHEL 7.  Unless you have an operational need, I would suggest basing your system off of CentOS 7.2 (build 1511), as that is where the bulk of the testing of this has happened.</p>

<p><strong>BE ADVISED:</strong>  This build process takes 3-10 minutes depending on your underlying hardware.  There will be times where it seems like it quit.  Be patient.  You'll know when it's done, for better or worse.</p>

<h3>
<a id="vagrant" class="anchor" href="#vagrant" aria-hidden="true"><span class="octicon octicon-link"></span></a>Vagrant</h3>

<p><strong>NOTE:</strong><br>
This Vagrantfile is configured to give the VM 8GB of RAM.  If your system can't do that you should buy a new system or adjust the <code>vm.memory</code> value.  Anything below 4 is going to run like poopoo.</p>

<pre><code>git clone https://github.com/CyberAnalyticDevTeam/SimpleRock.git
cd SimpleRock
vagrant up
</code></pre>

<h3>
<a id="physicalvirtualnon-vagrant" class="anchor" href="#physicalvirtualnon-vagrant" aria-hidden="true"><span class="octicon octicon-link"></span></a>Physical/Virtual/Non-Vagrant</h3>

<p><strong>NOTE:</strong><br>
The system you run this on should have at least 2 network interfaces and more than 4GB of RAM, with an OS (RHEL or CentOS 7) already installed.</p>

<pre><code>sudo rpm -Uvh https://bintray.com/artifact/download/cyberdev/capes/chef-12.3.0-1.el6.x86_64.rpm
sudo yum install git -y
git clone https://github.com/CyberAnalyticDevTeam/SimpleRock.git
cd SimpleRock
sudo chef-client -z -r "recipe[simplerock]"
</code></pre>

<h2>
<a id="minimum-hardware-recommendations" class="anchor" href="#minimum-hardware-recommendations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Minimum Hardware Recommendations</h2>

<h4>
<a id="for-anything-other-than-a-vagrant-build" class="anchor" href="#for-anything-other-than-a-vagrant-build" aria-hidden="true"><span class="octicon octicon-link"></span></a>(For anything other than a Vagrant build)</h4>

<p><strong>NOTE:</strong> This is a shadow of a recommendation of a guideline.  Your mileage may vary.  No returns or refunds.</p>

<ul>
<li> CPU

<ul>
<li> 4 or more physical cores.<br>
</li>
</ul>
</li>
<li> Memory

<ul>
<li> 16GB (You can get away with 8GB, but it won't collect for long.)</li>
</ul>
</li>
<li> Storage

<ul>
<li> 256GB, with 200+ of that dedicated to <code>/data</code>. Honestly, throw everything you can at it.  The higher the IOPS the better.</li>
</ul>
</li>
<li> Network

<ul>
<li> The system needs at least 2 network interfaces, one for management and one for collection.</li>
</ul>
</li>
</ul>

<p><strong>GOLDEN RULE:</strong> If you throw hardware at it, ROCK will use it.  It will require some tuning to do so, but we'll be documenting that soon enough.</p>

<h2>
<a id="usage" class="anchor" href="#usage" aria-hidden="true"><span class="octicon octicon-link"></span></a>Usage</h2>

<h4>
<a id="start--stop--status" class="anchor" href="#start--stop--status" aria-hidden="true"><span class="octicon octicon-link"></span></a>Start / Stop / Status</h4>

<p>Accomplished with <code>rock_stop</code>, <code>rock_start</code>, and <code>rock_status</code>.</p>

<p><strong>NOTE:</strong> These may need to be prefaced with /usr/local/bin/ depending on your PATH.</p>

<p><code>sudo rock_stop</code></p>

<pre><code>[root@simplerockbuild ~]# rock_stop
Stopping Bro...
stopping worker-1-1 ...
stopping worker-1-2 ...
stopping proxy-1 ...
stopping manager ...
Stopping Logstash...
Stopping Kibana...
Stopping Elasticsearch...
Stopping Kafka...
Stopping Zookeeper...
</code></pre>

<p><code>sudo rock_start</code></p>

<pre><code>[root@simplerockbuild ~]# rock_start
Starting Zookeeper...
   Active: active (running) since Wed 2015-12-02 17:12:02 UTC; 5s ago
Starting Elasticsearch...
   Active: active (running) since Wed 2015-12-02 17:12:07 UTC; 5s ago
Starting Kafka...
   Active: active (running) since Wed 2015-12-02 17:12:12 UTC; 5s ago
Starting Logstash...
   Active: active (running) since Wed 2015-12-02 17:12:17 UTC; 5s ago
Starting Kibana...
   Active: active (running) since Wed 2015-12-02 17:12:22 UTC; 5s ago
Starting Bro...
removing old policies in /data/bro/spool/installed-scripts-do-not-touch/site ...
removing old policies in /data/bro/spool/installed-scripts-do-not-touch/auto ...
creating policy directories ...
installing site policies ...
generating cluster-layout.bro ...
generating local-networks.bro ...
generating broctl-config.bro ...
generating broctl-config.sh ...
updating nodes ...
manager scripts are ok.
proxy-1 scripts are ok.
worker-1-1 scripts are ok.
worker-1-2 scripts are ok.
starting manager ...
starting proxy-1 ...
starting worker-1-1 ...
starting worker-1-2 ...
Getting process status ...
Getting peer status ...
Name         Type    Host             Status    Pid    Peers  Started
manager      manager localhost        running   20389  ???    02 Dec 17:12:34
proxy-1      proxy   localhost        running   20438  ???    02 Dec 17:12:35
worker-1-1   worker  localhost        running   20484  ???    02 Dec 17:12:36
worker-1-2   worker  localhost        running   20485  ???    02 Dec 17:12:36
</code></pre>

<p><code>sudo rock_status</code></p>

<pre><code>[root@simplerockbuild ~]# rock_status
Zookeeper...
   Active: active (running) since Wed 2015-12-02 17:12:02 UTC; 2min 7s ago
Elasticsearch...
   Active: active (running) since Wed 2015-12-02 17:12:07 UTC; 2min 2s ago
Kafka...
   Active: active (running) since Wed 2015-12-02 17:12:12 UTC; 1min 57s ago
Logstash...
   Active: active (running) since Wed 2015-12-02 17:12:17 UTC; 1min 52s ago
Kibana...
   Active: active (running) since Wed 2015-12-02 17:12:22 UTC; 1min 47s ago
Bro...
Getting process status ...
Getting peer status ...
Name         Type    Host             Status    Pid    Peers  Started
manager      manager localhost        running   20389  ???    02 Dec 17:12:34
proxy-1      proxy   localhost        running   20438  ???    02 Dec 17:12:35
worker-1-1   worker  localhost        running   20484  ???    02 Dec 17:12:36
worker-1-2   worker  localhost        running   20485  ???    02 Dec 17:12:36
Stenographer...
   Active: active (running) since Wed 2015-12-02 17:12:22 UTC; 1min 47s ago
</code></pre>

<h2>
<a id="basic-troubleshooting" class="anchor" href="#basic-troubleshooting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Basic Troubleshooting</h2>

<h4>
<a id="functions-check" class="anchor" href="#functions-check" aria-hidden="true"><span class="octicon octicon-link"></span></a>Functions Check:</h4>

<pre><code># After the initial build, the ES cluster will be yellow because the marvel index will think it's missing a replica.  Run this to fix this issue.  This job will run from cron just after midnight every day.
/usr/local/bin/es_cleanup.sh

# Check to see that the ES cluster says it's green:
curl -s localhost:9200/_cluster/health | jq '.'

# See how many documents are in the indexes.  The count should be non-zero.
curl -s localhost:9200/_all/_count | jq '.'

# You can fire some traffic across the sensor at this point to see if it's collecting.
# NOTE: This requires that you upload your own test PCAP to the box.
sudo tcpreplay -i [your monitor interface] /path/to/a/test.pcap

# After replaying some traffic, or just waiting a bit, the count should be going up.
curl -s localhost:9200/_all/_count | jq '.'

# You should have plain text bro logs showing up in /data/bro/logs/current/:
ls -ltr /data/bro/logs/current/

# Kafkacat is your kafka swiss army knife.  This command will consume the current queue.  You should see a non-zero offset.
kafkacat -C -b localhost -t bro_raw -e | wc -l

# If you haven't loaded kibana already, it should be running on port 5601.  This just verifies while you're still on the command line.
sudo netstat -planet | grep node
</code></pre>

<h2>
<a id="key-web-interfaces" class="anchor" href="#key-web-interfaces" aria-hidden="true"><span class="octicon octicon-link"></span></a>Key web interfaces:</h2>

<p>IPADDRESS = The management interface of the box, or "localhost" if you did the vagrant build.</p>

<p>http://IPADDRESS:5601 - Kibana</p>

<p>http://IPADDRESS:9200/_plugin/marvel - Marvel (To watch the health of elasticsearch.)</p>

<p>http://IPADDRESS:9200/_plugin/sql - Query your ES data with SQL.</p>

<h2>
<a id="full-packet-capture" class="anchor" href="#full-packet-capture" aria-hidden="true"><span class="octicon octicon-link"></span></a>Full Packet Capture</h2>

<p>Google's Stenographer is installed and configured in this build.  However, it is disabled by default.  There are a few reasons for this: First, it can be too much for Vagrant builds on meager hardware.  Second, you really need to make sure you've mounted /data over sufficient storage before you start saving full packets.  Once you're ready to get nuts, enable and start the service with <code>systemctl enable stenographer.service</code> and then <code>systemctl start stenographer.service</code>.  Stenographer is already stubbed into the <code>/usr/local/bin/rock_{start,stop,status}</code> scripts, you just need to uncomment it if you're going to use it. </p>

<h2>
<a id="thanks" class="anchor" href="#thanks" aria-hidden="true"><span class="octicon octicon-link"></span></a>THANKS</h2>

<p>This architecture is made possible by the efforts of the Missouri National Guard Cyber Team, and especially Critical Stack and BroEZ for donating talent and resources to further development.</p>

<h2>
<a id="approach" class="anchor" href="#approach" aria-hidden="true"><span class="octicon octicon-link"></span></a>Approach</h2>

<p>The Chef recipe that drives this build strives not to use external recipes and cookbooks where possible.  The reasoning behind this is to make the simplerock recipe a "one-stop" reference for a manual build.  This allows users to use the build process as a guide when doing larger scale production roll outs without having to decypher a labrynth of dependencies.</p>

<p>Templated config files have comment sections added near key config items with useful info.  They don't all have it, but they get added as remembered.</p>
        </section>

        <footer>
          SimpleRock is maintained by <a href="https://github.com/CyberAnalyticDevTeam">CyberAnalyticDevTeam</a><br>
          This page was generated by <a href="https://pages.github.com">GitHub Pages</a>. Tactile theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.
        </footer>

        
      </div>
    </div>
  </body>
</html>
